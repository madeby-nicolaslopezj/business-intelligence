########## hierarchical clustering  ############################################
d <- dist(iris[,-5], method = "euclidean") # proximity matrix (distance)

fit <- hclust(d, method="single") #other methods:
# "ward","complete", "average","mcquitty", "median", o "centroid".

plot(fit) # display dendogram
plot(fit,label=iris$Species)

# cut the tree for 3 clusters
groups <- cutree(fit, k=3)

# Dendrogram with the three clusters inside a red box
rect.hclust(fit, k=3, border="red") 

#Sampling
idx<-sample(1:dim(iris)[1],40)
irisMuestra<-iris[idx,]
d <- dist(irisMuestra[,-5], method = "euclidean") 

fit <- hclust(d, method="ward")
plot(fit)
plot(fit,label=irisMuestra$Species)
groups <- cutree(fit, k=3)
rect.hclust(fit, k=3, border="red") 

########## GMM with EM algorithm  ############################################

library(mclust)           # load mclust library
x = faithful[,1]          # first column of faithful dataset
y = faithful[,2]          # second column of faithful dataset
plot(x,y)                 # scatter plot
model <- Mclust(faithful) # number of cluster estimated by (BIC), initializes using (HC) and then perfoms clustering using GMM by EM

plot(model)     # plot the clustering results

summary(model,data=faithful) # displays all the information about the generated model.

model$classification # labels of the clusters

model$loglik #log-likelihood

out1 <- data.frame(cbind(faithful, clusterNum = model$classification)) # original data plus the column with the cluster labels

#mixture of two Gaussian distributions

x1 = rnorm(n=20, mean=1, sd=1)   # 20 random points generated by a standard normal distirbution (1st class)

y1 = rnorm(n=20, mean=1, sd=1)   # 20 random points generated by a standard normal distirbution (1st class)

x2 = rnorm(n=20, mean=5, sd=1)   # 20 random points generated by a standard normal distirbution (2nd class)

y2 = rnorm(n=20, mean=5, sd=1)   # 20 random points generated by a standard normal distirbution (2nd class)

rx = range(x1,x2)                # x axis range
ry = range(y1,y2)                # y axis range

plot(x1, y1, xlim=rx, ylim=ry)   # plots of points from the first class

points(x2, y2)  # plots of points from the second class

mix = matrix(nrow=40, ncol=2)    # create input matrix

#filling in the matrix
mix[,1] = c(x1, x2)              
mix[,2] = c(y1, y2)

mixclust = Mclust(mix)       # perfom clustering using EM

plot(mixclust)       # results

summary (mixclust)

########## cluster validity  ############################################

# Silhouette

# Example using kmeans, although any clustering technique may be used

library(clusterSim)
library(clValid)
datos<-iris[,-5]
distancia <- dist(iris[,-5], method = "euclidean")
# nc - number_of_clusters
min_nc=2
max_nc=8
res <- array(0, c(max_nc-min_nc+1,2))
res[,1] <- min_nc:max_nc
clusters <- NULL
for (nc in min_nc:max_nc)
{
  cl2 <- kmeans(datos,nc)
  res[nc-min_nc+1, 2] <- S <- index.S(distancia,cl2$cluster)
  clusters <- rbind(clusters, cl2)
}

#plot

# get the range for the x and y axis
xrange <- range(res[,1])
yrange <- range(res[,2])

# set up the plot
plot(xrange, yrange, type="n", xlab="Number of clusters",
     ylab="Silhouette values" )


# add lines

  lines(res[,1], res[,2], type="b", lwd=1.5,
        lty=1, col=4, pch=1)


# add a title
title("Results using Silhouette index")


# rand index
library(flexclust)
clusterResult=clusters[2,]
randIndex(clusterResult$cluster,iris[,5], correct=FALSE)

